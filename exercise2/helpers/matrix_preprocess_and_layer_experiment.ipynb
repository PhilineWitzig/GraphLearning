{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from data_utils import get_adjacency_matrix,get_node_attributes,get_node_labels\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataset_parser import Parser\n",
    "\n",
    "def load_datasets_nodelabel(names):\n",
    "    \"\"\"\n",
    "    Loads the graph datasets DD, ENZYMES and NCI1, CiteSeer_Eval,CiteSeer_Train,Cora_Eval,Cora_Train and its labels.\n",
    "    :params:    list of dataset names to load\n",
    "\n",
    "    :return:    list of dataset names, list of loaded graphs for all datasets,\n",
    "                node attributes for loaded graphs for all datasets\n",
    "    \"\"\"\n",
    "\n",
    "    # load datasets\n",
    "    datasets = []\n",
    "    if \"cs_eval\" in names:\n",
    "        datasets.append(Parser('datasets/CiteSeer_Eval'))\n",
    "    if \"cs_train\" in names:\n",
    "        datasets.append(Parser('datasets/CiteSeer_Train'))\n",
    "    if \"co_eval\" in names:\n",
    "        datasets.append(Parser('datasets/Cora_Eval'))\n",
    "    if \"co_train\" in names:\n",
    "        datasets.append(Parser('datasets/Cora_Train'))\n",
    "\n",
    "    # convert datasets into lists graphs, labels\n",
    "    datasets = [dataset.parse_all_graphs() for dataset in datasets]\n",
    "    attr_sets = [[get_node_attributes(graph) for graph in graphs] for graphs in datasets]\n",
    "    labels = [[get_node_labels(graph) for graph in graphs] for graphs in datasets]\n",
    "    #attr_sets is a list of length n, where n is the number of datasets. Then attr[0] contains a list of all node attributes \n",
    "    #for dataset 0. Thus attr[0][0] contains the actual node attribute matrix (X^0) for the graph of fataset 0.\n",
    "    return names, datasets, attr_sets, labels\n",
    "\n",
    "def f(x):\n",
    "    '''\n",
    "    Auxilliary function implementing degree normalization\n",
    "    \n",
    "    :param x: Float Value\n",
    "    :return: normalized value\n",
    "    '''\n",
    "    return np.divide(1,np.sqrt(x))\n",
    "\n",
    "def preprocess_matrices(datasets):\n",
    "    '''\n",
    "    Function implementing matreix normalization of adjecency matrices for all given graphs\n",
    "    \n",
    "    :param datasets: Array of datasets, each being a list of a single networkX graph\n",
    "    :return: Array of normalized adjecency matrices, index 0 holding the matrix for the first data set and so on.\n",
    "    '''\n",
    "    \n",
    "    matrices = []\n",
    "    for i,dataset in enumerate(datasets):\n",
    "        ## get the adjecency matrix and add the identity matrix to it\n",
    "        n = np.size(get_adjacency_matrix(dataset[0]),0) # dimension of adj.matrix\n",
    "        adj_matrix_self_loops = np.add(get_adjacency_matrix(dataset[0]), np.identity(n)) #add self loops by identity matrix\n",
    "        #compute diagonal matrix of degrees D\n",
    "        D_diagonal = adj_matrix_self_loops.sum(axis = 1) #get list of node degrees\n",
    "        D_diagonal = np.array(list(map(f, D_diagonal))) #normalization\n",
    "        D=np.zeros((n,n)) #make nxn matrix filled with zeros\n",
    "        np.fill_diagonal(D,D_diagonal) #fill the diagonal with normalized entries, obtainting root inverse D\n",
    "        \n",
    "        ##Symmetric normalization of adj.matrix\n",
    "        A = np.matmul(adj_matrix_self_loops,D)\n",
    "        A = np.matmul(D,A)\n",
    "        \n",
    "        matrices.append(A)#collect all computed matrices\n",
    "        \n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains the implementation of the GNN architectures.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class GCN(layers.Layer):\n",
    "    \"\"\"\n",
    "    Call function takes a list of two tensors as input.\n",
    "    The first entry  is the last node embedding.\n",
    "    The second is the normalized adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_num, nl):\n",
    "        \"\"\"\n",
    "        :param feature_num: number of features this layers outputs.\n",
    "        :param nl: non-linearity which is applied after convoluting.\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        self.feature_num = feature_num\n",
    "        self.nl = nl\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        w_init = tf.initializers.GlorotUniform()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_shape[0][-1], self.feature_num), dtype='float32'), trainable=True)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if type(inputs) is not list or not len(inputs) == 2:\n",
    "            raise Exception('GCN must be called on a list of two tensors. Got: ' + str(inputs))\n",
    "        X = inputs[0]\n",
    "        A = inputs[1]\n",
    "        x = tf.matmul(A, X)\n",
    "        x = tf.matmul(x, self.w)\n",
    "        if self.nl == \"relu\":\n",
    "            x = tf.nn.relu(x)\n",
    "        elif self.nl == \"softmax\":\n",
    "            x = tf.nn.softmax(x)\n",
    "        else:\n",
    "            raise ValueError(f\"GCN Layer only supports 'relu' and 'softmax' as non linearity, but {self.nl} was passed.\")\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0][1], self.feature_num\n",
    "\n",
    "\n",
    "class SumPool(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(GCN, self).__init__()\n",
    "        self.no_units = units\n",
    "\n",
    "    def build(self):\n",
    "        # TODO: check if this operation is valid\n",
    "        self.w = tf.Variable(trainable=False)\n",
    "\n",
    "    def call(self, node_features):\n",
    "        # TODO: check if this operation is valid or if we need to do it by hand\n",
    "        # In this case we will reuqire the no_units attribute\n",
    "        return tf.math.reduce_sum(node_features, axis=0)\n",
    "\n",
    "\n",
    "def model_GCN_node(v, k0, num_classes):\n",
    "    \"\"\"\n",
    "    Model architecture of GCN for node classification.\n",
    "\n",
    "    :param v: number of vertices in graphs\n",
    "    :param k0: number of node features\n",
    "    :param num_classes: nummer of classes to classify\n",
    "\n",
    "    :return: instance of tf.keras.Model implementing the GCN node model architecture\n",
    "    \"\"\"\n",
    "    # input layers, depending on size of node_features and\n",
    "    node_features_input_layer = layers.Input(shape=(v, k0), name='node_feature_input')\n",
    "    adjacency_matrix_input_layer = layers.Input(shape=(v, v), name='adjacency_matrix_input')\n",
    "    x = GCN(32, \"relu\")([node_features_input_layer, adjacency_matrix_input_layer])\n",
    "    x = GCN(num_classes, \"softmax\")([x, adjacency_matrix_input_layer])\n",
    "\n",
    "    model = tf.keras.Model(inputs=[node_features_input_layer, adjacency_matrix_input_layer], outputs=[x],\n",
    "                           name='GCN_Graph')\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_GCN_graph(v, k0, num_classes):\n",
    "    \"\"\"\n",
    "    Model architecture of GCN for graph classification.\n",
    "\n",
    "    :param v:           number of vertices in the graph\n",
    "    :param k0:          number of node features\n",
    "    :param num_classes: number of graph classes to classify\n",
    "    \"\"\"\n",
    "    node_features_input_layer = layers.Input(shape=(v, k0),\n",
    "                                             name='node_feature_input')\n",
    "    adjacency_matrix_input_layer = layers.Input(shape=(v, v),\n",
    "                                                name='adjacency_matrix_input')\n",
    "    gcn_1 = GCN(64, \"relu\")(something)\n",
    "    gcn_2 = GCN(64, \"relu\")([gcn_1, adjacency_matrix_input_layer])\n",
    "    gcn_3 = GCN(64, \"relu\")([gcn_2, adjacency_matrix_input_layer])\n",
    "    gcn_4 = GCN(64, \"relu\")([gcn_3, adjacency_matrix_input_layer])\n",
    "    gcn_5 = GCN(64, \"relu\")([gcn_4, adjacency_matrix_input_layer])\n",
    "    sum_pool = SumPool(64)([gcn_5])\n",
    "    fc_1 = layers.Dense(input=sum_pool, units=64, activation=\"relu\")\n",
    "    fc_2 = layers.Dense(input=fc_1, units=num_classes)\n",
    "    softmax = layers.softmax(fc_2)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[node_features_input_layer, adjacency_matrix_input_layer],\n",
    "                           outpus=[softmax])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) \n",
    "#supresses warnings regarding float64 to float32 conversion for better readability\n",
    "\n",
    "#We use Cross Entropy for our loss\n",
    "loss_object= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def loss(model, x, y, training):\n",
    "    '''\n",
    "    Function returning the loss of the given model.\n",
    "    Uses the predefined loss of loss_object (which is Categocial Cross Entropy)\n",
    "    \n",
    "    :param model: Tensorflow Neural Network\n",
    "    :param x: Feature data\n",
    "    :param y: True Labels for the given feature data\n",
    "    '''\n",
    "  # training=training is needed only if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    y_ = model(x, training=training)\n",
    "    return loss_object(y, y_)\n",
    "\n",
    "\n",
    "def grad(model, x, y):\n",
    "    '''\n",
    "    Function calculating the gradient of the given model.\n",
    "    Uses the predefined loss of loss_object (which is Categocial Cross Entropy)\n",
    "    \n",
    "    :param model: Tensorflow Neural Network\n",
    "    :param x: Feature data\n",
    "    :param y: True Labels for the given feature data\n",
    "    \n",
    "    :return: Loss and Gradient\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, training=True)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "\n",
    "#Define Optimizer and learning rate\n",
    "#TODO: Find optimal learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "def train_model(num_epochs, model, features, labels):\n",
    "    '''\n",
    "    Function to train the given model. Will print accuracy and loss every 50 epochs.\n",
    "    Uses a batch size of one and predefined learning rate given by the predefined optimizer.\n",
    "    The optimizer is chosen to be Adam.\n",
    "    \n",
    "    :param num_epochs: Number of epochs\n",
    "    :param model: Tensorflow Neural Network which should be trained\n",
    "    :param features: Input for the Neural Network, given as list of inputs. The first entry \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "# Keep results for plotting\n",
    "    train_loss_results = []\n",
    "    train_accuracy_results = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        # Optimize the model\n",
    "        loss_value, grads = grad(model, features, labels)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Track progress\n",
    "        epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
    "        # Compare predicted label to actual label\n",
    "        # training=True is needed only if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        epoch_accuracy.update_state(labels, model(features, training=True))\n",
    "\n",
    "      # End epoch\n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "        train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                    epoch_loss_avg.result(),\n",
    "                                                                    epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Peprocessing data\n",
      "1656\n",
      "[[1.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.5 0.  ... 0.  0.  0. ]\n",
      " [0.  0.  1.  ... 0.  0.  0. ]\n",
      " ...\n",
      " [0.  0.  0.  ... 1.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.  1.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.  1. ]]\n",
      "Matrix normalization completed\n",
      "Beginning training for dataset cs_train\n",
      "1656\n",
      "[[1.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.5 0.  ... 0.  0.  0. ]\n",
      " [0.  0.  1.  ... 0.  0.  0. ]\n",
      " ...\n",
      " [0.  0.  0.  ... 1.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.  1.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.  1. ]]\n",
      "Epoch 000: Loss: 1.947, Accuracy: 63.164%\n",
      "Epoch 050: Loss: 1.243, Accuracy: 92.452%\n",
      "Epoch 100: Loss: 1.234, Accuracy: 93.056%\n",
      "Epoch 150: Loss: 1.232, Accuracy: 93.176%\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 200\n",
    "\n",
    "def run_node_label_training(names):\n",
    "    '''\n",
    "    Function used to train models on specified datasets. The function will automatically train the node classifying model \n",
    "    on the training data associated with the given name for a fixed amount of epochs, printing the accuracy and loss per 50\n",
    "    epochs.\n",
    "    \n",
    "    :param names: List of Strings representing the data sets. Options: \"CiteSeer\" or \"Cora\". e.g. [\"CiteSeer\"]. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    load_datasets_names = []\n",
    "    if \"CiteSeer\" in names:\n",
    "        load_datasets_names.append('cs_train')\n",
    "    if \"Cora\" in names:\n",
    "        load_datasets_names.append('co_train')\n",
    "        \n",
    "    if not load_datasets_names:\n",
    "        print(\"Please specify a dataset. Options are CiteSeer and Cora\")\n",
    "    \n",
    "    for name in load_datasets_names:\n",
    "        _, data, attr, labels = load_datasets_nodelabel([name])\n",
    "        print(\"Currently Peprocessing data\")\n",
    "        normalized_adj_matrix = preprocess_matrices(data)[0]\n",
    "        print(\"Matrix normalization completed\")\n",
    "        \n",
    "        print(\"Beginning training for dataset\", name)\n",
    "        \n",
    "        number_of_features = len(attr[0][0][0])\n",
    "        number_of_nodes = len(attr[0][0])\n",
    "        number_of_labels = np.amax(labels)\n",
    "        \n",
    "        normalized_adj_matrix = tf.convert_to_tensor(preprocess_matrices(data)[0]) #A\n",
    "        node_attributes = tf.convert_to_tensor(attr[0][0])\n",
    "        model = model_GCN_node(number_of_nodes,number_of_features, (number_of_labels+1)) \n",
    "        #TODO: check if +1 is valid or just a stupid quick fix\n",
    "        #by default the labels range [0,number_of_labels), since we range [1,number_of_labels] we add 1 instead.\n",
    "        \n",
    "        features = [node_attributes, normalized_adj_matrix]\n",
    "        labels_ = labels[0][0]\n",
    "        \n",
    "        train_model(NUM_EPOCHS, model, features, labels_)\n",
    "        \n",
    "#EXAMPLE:\n",
    "#run_node_label_training([\"CiteSeer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
